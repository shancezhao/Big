d <- dist(t(dtmss), method="euclidian")
fit <- hclust(d=d, method="ward.D")
plot(fit,hang=-1)
plot.new()
plot(fit,hang = -1)
groups <- cutree(fit,k=5)
rect.hclust(fit,k=5,border="red")
library(fpc)
d <- dist(t(dtmss),method="euclidian")
kfit <- kmeans(d,2)
kfit <- kmeans(d,2)
clusplot(as.matrix(d),kfit$cluster,color=T,shade=T,labels=2,lines=0)
cname <- file.path("C:/Users/zhaos/Desktop/texts","odyssey")
cname
library(tm)
docs <- Corpus(DirSource(cname))
summary(docs)
docs <- tm_map(docs,removePunctuation)
##remove the numbers
docs <- tm_map(docs,removeNumbers)
##convert to lowercase
docs <- tm_map(docs,tolower)
##remove "stopwords"
docs <- tm_map(docs,removeWords,stopwords("english"))
## Remove your own stop word
docs <- tm_map(docs,removeWords,c("balabala","yoyo"))
docs <- tm_map(docs,stemDocument)
##Stripping unnecesary whitespace from your documents
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs,PlainTextDocument)
##create a document term matrix
dtm <- DocumentTermMatrix(docs)
dtm
tdm <- TermDocumentMatrix(docs)
tdm
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
##export the matrix to Excel
m <- as.matrix(dtm)
dim(m)
write.csv(m,file="dtmodyss.csv")
dtms <- removeSparseTerms(dtm,0.1)
inspect(dtms)
freq[head(ord)]
freq[tail(ord)]
head(table(freq),20)
tail(table(freq),20)
freq <- colSums(as.matrix(dtms))
freq
freq <- sort(colSums(as.matrix(dtm)),decreasing = TRUE)
head(freq,14)
findFreqTerms(dtm, lowfreq = 300)
wf <- data.frame(word=names(freq),freq=freq)
head(wf)
##plot word frequencies
library(ggplot2)
p <- ggplot(subset(wf,freq>500),aes(word,freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x = element_text(angle=45, hjust=1))
p
wf <- data.frame(word=names(freq),freq=freq)
head(wf)
##plot word frequencies
library(ggplot2)
p <- ggplot(subset(wf,freq>200),aes(word,freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x = element_text(angle=45, hjust=1))
p
findAssocs(dtm, c("god", "back"),corlimit = 0.98)
findAssocs(dtms, "contract",corlimit = 0.90)
findAssocs(dtms, "man",corlimit = 0.90)
library(wordcloud)
set.seed(142)
wordcloud(names(freq),freq,min.freq = 250)
set.seed(142)
wordcloud(names(freq),freq,max.words = 100)
set.seed(142)
wordcloud(names(freq),freq,min.freq=250, scale=c(5, .1),colors=brewer.pal(6, "Dark2"))
set.seed(142)
dark2 <- brewer.pal(6, "Dark2")
wordcloud(names(freq),freq, max.words = 100, rot.per = 0.2, colors=dark2)
dtmss <- removeSparseTerms(dtm, 0.15)
inspect(dtmss)
library(cluster)
d <- dist(t(dtmss), method="euclidian")
fit <- hclust(d=d, method="ward.D")
plot(fit,hang=-1)
plot.new()
plot(fit,hang = -1)
groups <- cutree(fit,k=5)
rect.hclust(fit,k=5,border="red")
library(fpc)
d <- dist(t(dtmss),method="euclidian")
kfit <- kmeans(d,2)
kfit <- kmeans(d,1)
kfit <- kmeans(d,2)
clusplot(as.matrix(d),kfit$cluster,color=T,shade=T,labels=2,lines=0)
library(tm)
library(SnowballC)
library(RColorBrewer)
library(ggplot2)
library(wordcloud)
library(superbiclust)
library(cluster)
library(igraph)
library(fpc)
cname <- file.path("C:/Users/zhaos/Desktop/texts","iliad")
cname
dir(cname)
library(tm)
docs <- Corpus(DirSource(cname))
docs <- tm_map(docs,removePunctuation)
docs <- tm_map(docs,removeNumbers)
docs <- tm_map(docs,tolower)
docs <- tm_map(docs,removeWords,stopwords("english"))
library(SnowballC)
docs <- tm_map(docs, stemDocument)
docs <- tm_map(docs,stemDocument)
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs,PlainTextDocument)
dtm <- DocumentTermMatrix(docs)
tdm <- TermDocumentMatrix(docs)
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
m <- as.matrix(dtm)
dim(m)
write.csv(m,file="DocumentTermMatrix.csv")
dtms <- removeSparseTerms(dtm, 0.1)
head(table(freq),20)
tail(table(freq),20)
freq <- colSums(as.matrix(dtms))
freq
findFreqTerms(dtm, lowfreq = 300)
library(ggplot2)
wf <- data.frame(word=names(freq),freq=freq)
p <- ggplot(subset(wf,freq>500),aes(word,freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x = element_text(angle=45, hjust=1))
p
wf <- data.frame(word=names(freq),freq=freq)
p <- ggplot(subset(wf,freq>500),aes(word,freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x = element_text(angle=45, hjust=1))
p
cname <- file.path("C:/Users/zhaos/Desktop/texts","iliad")
cname
dir(cname)
library(tm)
docs <- Corpus(DirSource(cname))
docs <- tm_map(docs,removePunctuation)
##remove the numbers
docs <- tm_map(docs,removeNumbers)
##convert to lowercase
docs <- tm_map(docs,tolower)
##remove "stopwords"
docs <- tm_map(docs,removeWords,stopwords("english"))
library(SnowballC)
docs <- tm_map(docs, stemDocument)
##Removing common word endings (e.g., "ing", "es", "s")
docs <- tm_map(docs,stemDocument)
##Stripping unnecesary whitespace from your documents
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs,PlainTextDocument)
##stage the data
dtm <- DocumentTermMatrix(docs)
tdm <- TermDocumentMatrix(docs)
##explore data
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
m <- as.matrix(dtm)
dim(m)
write.csv(m,file="DocumentTermMatrix.csv")
dtms <- removeSparseTerms(dtm, 0.1)
head(table(freq),20)
tail(table(freq),20)
freq <- colSums(as.matrix(dtms))
freq
findFreqTerms(dtm, lowfreq = 300)
library(ggplot2)
wf <- data.frame(word=names(freq),freq=freq)
p <- ggplot(subset(wf,freq>500),aes(word,freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x = element_text(angle=45, hjust=1))
p
cname <- file.path("C:/Users/zhaos/Desktop/texts","iliad")
cname
dir(cname)
library(tm)
docs <- Corpus(DirSource(cname))
docs <- tm_map(docs,removePunctuation)
##remove the numbers
docs <- tm_map(docs,removeNumbers)
##convert to lowercase
docs <- tm_map(docs,tolower)
##remove "stopwords"
docs <- tm_map(docs,removeWords,stopwords("english"))
library(SnowballC)
docs <- tm_map(docs, stemDocument)
##Removing common word endings (e.g., "ing", "es", "s")
docs <- tm_map(docs,stemDocument)
##Stripping unnecesary whitespace from your documents
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs,PlainTextDocument)
##stage the data
dtm <- DocumentTermMatrix(docs)
tdm <- TermDocumentMatrix(docs)
##explore data
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
m <- as.matrix(dtm)
dim(m)
write.csv(m,file="DocumentTermMatrix.csv")
dtms <- removeSparseTerms(dtm, 0.1)
head(table(freq),20)
tail(table(freq),20)
freq <- colSums(as.matrix(dtms))
freq
findFreqTerms(dtm, lowfreq = 300)
library(ggplot2)
wf <- data.frame(word=names(freq),freq=freq)
p <- ggplot(subset(wf,freq>500),aes(word,freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x = element_text(angle=45, hjust=1))
p
Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust",
"cluster", "igraph", "fpc")
install.packages(Needed, dependencies=TRUE)
install.packages("Rcampdf", repos = "http://datacube.wu.ac.at/", type = "source")
install.packages(Needed, dependencies = TRUE)
install.packages(Needed, dependencies = TRUE)
cname <- file.path("C:/Users/zhaos/Desktop/texts","iliad")
cname
dir(cname)
library(tm)
docs <- Corpus(DirSource(cname))
docs <- tm_map(docs,removePunctuation)
##remove the numbers
docs <- tm_map(docs,removeNumbers)
##convert to lowercase
docs <- tm_map(docs,tolower)
##remove "stopwords"
docs <- tm_map(docs,removeWords,stopwords("english"))
library(SnowballC)
docs <- tm_map(docs, stemDocument)
##Removing common word endings (e.g., "ing", "es", "s")
docs <- tm_map(docs,stemDocument)
##Stripping unnecesary whitespace from your documents
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs,PlainTextDocument)
##stage the data
dtm <- DocumentTermMatrix(docs)
tdm <- TermDocumentMatrix(docs)
##explore data
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
m <- as.matrix(dtm)
dim(m)
write.csv(m,file="DocumentTermMatrix.csv")
dtms <- removeSparseTerms(dtm, 0.1)
head(table(freq),20)
tail(table(freq),20)
freq <- colSums(as.matrix(dtms))
freq
findFreqTerms(dtm, lowfreq = 300)
library(ggplot2)
wf <- data.frame(word=names(freq),freq=freq)
p <- ggplot(subset(wf,freq>500),aes(word,freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x = element_text(angle=45, hjust=1))
p
filePath <- "C:/Users/zhaos/Desktop/texts/iliad.txt"
text <- readLines(filePath)
filePath <- "C:\Users\zhaos\Desktop\texts\iliad\iliad.txt"
text <- readLines(filePath)
filePath <- "C:/Users/zhaos/Desktop/texts/iliad/iliad.txt"
text <- readLines(filePath)
docs <- Corpus(VectorSource(text))
library(tm)
docs <- Corpus(DirSource(cname))
docs <- tm_map(docs,removePunctuation)
##remove the numbers
docs <- tm_map(docs,removeNumbers)
##convert to lowercase
docs <- tm_map(docs,tolower)
##remove "stopwords"
docs <- tm_map(docs,removeWords,stopwords("english"))
library(SnowballC)
docs <- tm_map(docs, stemDocument)
##Removing common word endings (e.g., "ing", "es", "s")
docs <- tm_map(docs,stemDocument)
##Stripping unnecesary whitespace from your documents
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs,PlainTextDocument)
##stage the data
dtm <- DocumentTermMatrix(docs)
tdm <- TermDocumentMatrix(docs)
##explore data
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
m <- as.matrix(dtm)
dim(m)
write.csv(m,file="DocumentTermMatrix.csv")
dtms <- removeSparseTerms(dtm, 0.1)
head(table(freq),20)
tail(table(freq),20)
freq <- colSums(as.matrix(dtms))
freq
findFreqTerms(dtm, lowfreq = 300)
library(ggplot2)
wf <- data.frame(word=names(freq),freq=freq)
p <- ggplot(subset(wf,freq>500),aes(word,freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x = element_text(angle=45, hjust=1))
p
cname <- file.path("C:/Users/zhaos/Desktop/texts","odyssey")
cname
dir(cname)
docs <- Corpus(VectorSource(text))
library(tm)
docs <- Corpus(DirSource(cname))
docs <- tm_map(docs,removePunctuation)
##remove the numbers
docs <- tm_map(docs,removeNumbers)
##convert to lowercase
docs <- tm_map(docs,tolower)
##remove "stopwords"
docs <- tm_map(docs,removeWords,stopwords("english"))
library(SnowballC)
docs <- tm_map(docs, stemDocument)
##Removing common word endings (e.g., "ing", "es", "s")
docs <- tm_map(docs,stemDocument)
##Stripping unnecesary whitespace from your documents
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs,PlainTextDocument)
##stage the data
dtm <- DocumentTermMatrix(docs)
tdm <- TermDocumentMatrix(docs)
##explore data
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
m <- as.matrix(dtm)
dim(m)
write.csv(m,file="DocumentTermMatrix.csv")
dtms <- removeSparseTerms(dtm, 0.1)
head(table(freq),20)
tail(table(freq),20)
freq <- colSums(as.matrix(dtms))
freq
findFreqTerms(dtm, lowfreq = 300)
library(ggplot2)
wf <- data.frame(word=names(freq),freq=freq)
p <- ggplot(subset(wf,freq>500),aes(word,freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x = element_text(angle=45, hjust=1))
p
cname <- file.path("C:/Users/zhaos/Desktop/texts","odyssey")
cname
dir(cname)
docs <- Corpus(VectorSource(text))
# **Load the R package for text mining and then load your texts into R.**
library(tm)
docs <- Corpus(DirSource(cname))
## Preprocessing
docs <- tm_map(docs, removePunctuation) # *Removing punctuation:*
docs <- tm_map(docs, removeNumbers) # *Removing numbers:*
docs <- tm_map(docs, tolower) # *Converting to lowercase:*
docs <- tm_map(docs, removeWords, stopwords("english")) # *Removing "stopwords"
library(SnowballC)
docs <- tm_map(docs, stemDocument) # *Removing common word endings* (e.g., "ing", "es")
docs <- tm_map(docs, stripWhitespace) # *Stripping whitespace
docs <- tm_map(docs, PlainTextDocument)
## *This is the end of the preprocessing stage.*
### Stage the Data
dtm <- DocumentTermMatrix(docs)
tdm <- TermDocumentMatrix(docs)
### Explore your data
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
m <- as.matrix(dtm)
dim(m)
write.csv(m, file="DocumentTermMatrix.csv")
### FOCUS - on just the interesting stuff...
# Start by removing sparse terms:
dtms <- removeSparseTerms(dtm, 0.1) # This makes a matrix that is 10% empty space, maximum.
### Word Frequency
head(table(freq), 20)
# The above output is two rows of numbers. The top number is the frequency with which
# words appear and the bottom number reflects how many words appear that frequently.
#
tail(table(freq), 20)
# Considering only the 20 greatest frequencies
#
# **View a table of the terms after removing sparse terms, as above.
freq <- colSums(as.matrix(dtms))
freq
# The above matrix was created using a data transformation we made earlier.
# **An alternate view of term frequency:**
# This will identify all terms that appear frequently (in this case, 50 or more times).
findFreqTerms(dtm, lowfreq=50) # Change "50" to whatever is most appropriate for your data.
#
#
#
### Plot Word Frequencies
# **Plot words that appear at least 50 times.**
library(ggplot2)
wf <- data.frame(word=names(freq), freq=freq)
p <- ggplot(subset(wf, freq>50), aes(word, freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
cname <- file.path("C:/Users/zhaos/Desktop","texts")
dir(cname)
cname <- file.path("C:/Users/zhaos/Desktop","texts")
dir(cname)
library(tm)
docs <- Corpus(DirSource(cname))
## Preprocessing
docs <- tm_map(docs, removePunctuation) # *Removing punctuation:*
docs <- tm_map(docs, removeNumbers) # *Removing numbers:*
docs <- tm_map(docs, tolower) # *Converting to lowercase:*
docs <- tm_map(docs, removeWords, stopwords("english")) # *Removing "stopwords"
library(SnowballC)
docs <- tm_map(docs, stemDocument) # *Removing common word endings* (e.g., "ing", "es")
docs <- tm_map(docs, stripWhitespace) # *Stripping whitespace
docs <- tm_map(docs, PlainTextDocument)
## *This is the end of the preprocessing stage.*
### Stage the Data
dtm <- DocumentTermMatrix(docs)
tdm <- TermDocumentMatrix(docs)
### Explore your data
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
m <- as.matrix(dtm)
dim(m)
write.csv(m, file="DocumentTermMatrix.csv")
### FOCUS - on just the interesting stuff...
# Start by removing sparse terms:
dtms <- removeSparseTerms(dtm, 0.1) # This makes a matrix that is 10% empty space, maximum.
### Word Frequency
head(table(freq), 20)
# The above output is two rows of numbers. The top number is the frequency with which
# words appear and the bottom number reflects how many words appear that frequently.
#
tail(table(freq), 20)
# Considering only the 20 greatest frequencies
#
# **View a table of the terms after removing sparse terms, as above.
freq <- colSums(as.matrix(dtms))
freq
# The above matrix was created using a data transformation we made earlier.
# **An alternate view of term frequency:**
# This will identify all terms that appear frequently (in this case, 50 or more times).
findFreqTerms(dtm, lowfreq=50)
library(ggplot2)
wf <- data.frame(word=names(freq), freq=freq)
p <- ggplot(subset(wf, freq>50), aes(word, freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
p <- ggplot(subset(wf, freq>500), aes(word, freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
findAssocs(dtm, c("heaven", "arm"),corlimit = 0.98)
library(wordcloud)
dtms <- removeSparseTerms(dtm, 0.15)
freq <- colSums(as.matrix(dtm))
dark2 <- brewer.pal(6, "Dark2")
wordcloud(names(freq), freq, max.words=100, rot.per=0.2, colors=dark2)
dtms <- removeSparseTerms(dtm, 0.15) # This makes a matrix that is only 15% empty space.
library(cluster)
d <- dist(t(dtms), method="euclidian") # First calculate distance between words
fit <- hclust(d=d, method="ward")
dtms <- removeSparseTerms(dtm, 0.15) # This makes a matrix that is only 15% empty space.
library(cluster)
d <- dist(t(dtms), method="euclidian") # First calculate distance between words
fit <- hclust(d=d, method="ward.D2")
plot.new()
plot(fit, hang=-1)
groups < cutree(fit, k=5) # "k=" defines the number of clusters you are using
rect.hclust(fit, k=5, border="red")
plot.new()
plot(fit, hang=-1)
groups <- cutree(fit, k=5) # "k=" defines the number of clusters you are using
rect.hclust(fit, k=5, border="red")
dtms <- removeSparseTerms(dtm, 0.15) # This makes a matrix that is only 15% empty space.
library(cluster)
d <- dist(t(dtms), method="euclidian") # First calculate distance between words
fit <- hclust(d=d, method="ward.D2")
plot.new()
plot(fit, hang=-1)
groups <- cutree(fit, k=5) # "k=" defines the number of clusters you are using
rect.hclust(fit, k=5, border="red")
library(fpc)
library(cluster)
dtms <- removeSparseTerms(dtm, 0.15) # Prepare the data (max 15% empty space)
d <- dist(t(dtms), method="euclidian")
kfit <- kmeans(d, 2)
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igrap
h", "fpc")
install.packages(Needed, dependencies=TRUE)
install.packages("Rcampdf", repos = "http://datacube.wu.ac.at/", type = "source")
